{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science Boot Camp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Pandas Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Wrangling with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Now that we have been exposed to the basic functionality of Pandas, lets explore some more advanced features that will be useful when addressing more complex data management tasks.<br>\n",
    "<br>\n",
    "* As most statisticians/data analysts will admit, often the lion's share of the time spent implementing an analysis is devoted to preparing the data itself, rather than to coding or running a particular model that uses the data.<br>\n",
    "<br>\n",
    "* This is where Pandas and  Python's standard library are beneficial, providing high-level, flexible, and efficient tools for manipulating your data as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set some Pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Date/Time data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Date and time data are inherently problematic:<br>\n",
    "    * There are an unequal number of days in every month, an unequal number of days in a year (due to leap years)<br>\n",
    "    <br>\n",
    "    * Time zones that vary over space.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But information about time is essential in many analyses, particularly in the case of time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The `datetime` built-in library handles temporal information down to the nanosecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 4, 9, 12, 11, 21, 285235)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now.weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In addition to `datetime` there are simpler objects for date and time information only, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.time(3, 24)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time(3, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(1987, 8, 8)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date(1987, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Having a custom data type for dates and times is convenient because we can perform operations on them easily. For example, we may want to calculate the difference between two times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(11202, 43881, 285235)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_age = now - datetime(1987, 8, 8)\n",
    "my_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.69041095890411"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_age.days/365."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merging and joining DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data contained in pandas objects can be combined together in a number of built-in ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `pandas.merge` connects rows in DataFrames based on one or more keys. This will be familiar to users of SQL or other relational databases, as it implements database join operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `pandas.concat` glues or stacks together objects along an axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `combine_first` instance method enables splicing together overlapping data to fill\n",
    "in missing values in one object with values from another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Database-style DataFrame Merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Merge or join operations combine data sets by linking rows using one or more keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data1 key\n",
      "0      0   b\n",
      "1      1   b\n",
      "2      2   a\n",
      "3      3   c\n",
      "4      4   a\n",
      "5      5   a\n",
      "6      6   b\n",
      "   data2 key\n",
      "0      0   a\n",
      "1      1   b\n",
      "2      2   d\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1 key  data2\n",
       "0      0   b      1\n",
       "1      1   b      1\n",
       "2      6   b      1\n",
       "3      2   a      0\n",
       "4      4   a      0\n",
       "5      5   a      0"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Note that I didn’t specify which column to join on. If not specified, merge uses the overlapping column names as the keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the column names are different in each object, you can specify them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1 lkey  data2 rkey\n",
       "0      0    b      1    b\n",
       "1      1    b      1    b\n",
       "2      6    b      1    b\n",
       "3      2    a      0    a\n",
       "4      4    a      0    a\n",
       "5      5    a      0    a"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "pd.merge(df3, df4, left_on='lkey', right_on='rkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* You probably noticed that the 'c' and 'd' values and associated data are missing from the result.<br>\n",
    "<br>\n",
    "* By default merge does an 'inner' join; the keys in the result are the intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Other possible options are 'left', 'right', and 'outer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1 key  data2\n",
       "0    0.0   b    1.0\n",
       "1    1.0   b    1.0\n",
       "2    6.0   b    1.0\n",
       "3    2.0   a    0.0\n",
       "4    4.0   a    0.0\n",
       "5    5.0   a    0.0\n",
       "6    3.0   c    NaN\n",
       "7    NaN   d    2.0"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df1, df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1 key  data2\n",
       "0      0   b    1.0\n",
       "1      1   b    1.0\n",
       "2      2   a    0.0\n",
       "3      3   c    NaN\n",
       "4      4   a    0.0\n",
       "5      5   a    0.0\n",
       "6      6   b    1.0"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df1, df2, on='key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data1 key  data2\n",
       "0    0.0   b      1\n",
       "1    1.0   b      1\n",
       "2    6.0   b      1\n",
       "3    2.0   a      0\n",
       "4    4.0   a      0\n",
       "5    5.0   a      0\n",
       "6    NaN   d      2"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df1, df2, on='key', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In some cases, the merge key or keys in a DataFrame will be found in its index. In this case, you can pass left_index=True or right_index=True (or both) to indicate that the index should be used as the merge key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   group_val\n",
       "a        3.5\n",
       "b        7.0"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data2 rkey  group_val\n",
       "0      0    a        3.5\n",
       "1      1    b        7.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df4, df5, left_on='rkey', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* DataFrame has a more convenient join instance for merging by index. \n",
    "* It can also be used to combine together many DataFrame objects having the same or similar indexes but non-overlapping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   data2 rkey  group_val\n",
       "a      0    a        3.5\n",
       "b      1    b        7.0\n",
       "d      2    d        NaN"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = df4.copy()\n",
    "df6.index = df6['rkey']\n",
    "df6.join(df5, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A common data manipulation is appending rows or columns to a dataset that already conform to the dimensions of the exsiting rows or colums, respectively. In NumPy, this is done either with `concatenate` `vstack`, `np.hstack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12158175, 0.09856507, 0.48095483, 0.23045218, 0.45922531,\n",
       "       0.99578848, 0.1138037 , 0.99722971, 0.11729559, 0.74239983])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.random.random(5), np.random.random(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3913351 , 0.54617912, 0.29700321, 0.68903758, 0.76088209,\n",
       "       0.81998056, 0.58324589, 0.7286154 , 0.37406024, 0.86195248])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.random.random(5), np.random.random(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* This operation is also called *binding* or *stacking*.<br>\n",
    "<br>\n",
    "* With Pandas' indexed data structures, there are additional considerations as the overlap in index values between two data structures affects how they are concatenate.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    0\n",
       "b    1\n",
       "c    2\n",
       "d    3\n",
       "e    4\n",
       "f    5\n",
       "g    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = pd.Series([0, 1], index=['a', 'b'])\n",
    "s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])\n",
    "s3 = pd.Series([5, 6], index=['f', 'g'])\n",
    "pd.concat([s1, s2, s3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If you pass axis=1, the result will instead be a DataFrame (axis=1 is the columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0    1    2\n",
       "a  0.0  NaN  NaN\n",
       "b  1.0  NaN  NaN\n",
       "c  NaN  2.0  NaN\n",
       "d  NaN  3.0  NaN\n",
       "e  NaN  4.0  NaN\n",
       "f  NaN  NaN  5.0\n",
       "g  NaN  NaN  6.0"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([s1, s2, s3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In this case there is no overlap on the other axis, which as you can see is the sorted union (the 'outer' join) of the indexes. You can instead intersect them by passing join='inner':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    0\n",
       "b    5\n",
       "f    5\n",
       "g    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = pd.concat([s1 * 5, s3])\n",
    "s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0  1\n",
       "a  0  0\n",
       "b  1  5"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([s1, s4], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* One issue is that the concatenated pieces are not identifiable in the result. Suppose instead you wanted to create a hierarchical index on the concatenation axis. To do this, use the keys argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         a    b    c    d    e    f    g\n",
       "one    0.0  1.0  NaN  NaN  NaN  NaN  NaN\n",
       "two    NaN  NaN  2.0  3.0  4.0  NaN  NaN\n",
       "three  NaN  NaN  NaN  NaN  NaN  5.0  6.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([s1, s2, s3], keys=['one', 'two', 'three'])\n",
    "result.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reshaping and Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are a number of fundamental operations for rearranging tabular data. These are alternatingly referred to as reshape or pivot operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reshaping with Hierarchical Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Hierarchical indexing provides a consistent way to rearrange data in a DataFrame. There are two primary actions:<br>\n",
    "    * stack: this “rotates” or pivots from the columns in the data to the rows<br>\n",
    "    <br>\n",
    "    * unstack: this pivots from the rows into the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one  two  three  four\n",
       "Istanbul    0    1      2     3\n",
       "Ankara      4    5      6     7\n",
       "Izmir       8    9     10    11"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                 index=['Istanbul', 'Ankara', 'Izmir'],\n",
    "                 columns=['one', 'two', 'three', 'four'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Using the stack method on this data pivots the columns into the rows, producing a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Istanbul  one       0\n",
       "          two       1\n",
       "          three     2\n",
       "          four      3\n",
       "Ankara    one       4\n",
       "          two       5\n",
       "          three     6\n",
       "          four      7\n",
       "Izmir     one       8\n",
       "          two       9\n",
       "          three    10\n",
       "          four     11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one  two  three  four\n",
       "Istanbul    0    1      2     3\n",
       "Ankara      4    5      6     7\n",
       "Izmir       8    9     10    11"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stack().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* By default the innermost level is unstacked (same with stack). You can unstack a different level by passing a level number or name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one  two  three  four\n",
       "Istanbul    0    1      2     3\n",
       "Ankara      4    5      6     7\n",
       "Izmir       8    9     10    11"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Istanbul  Ankara  Izmir\n",
       "one           0       4      8\n",
       "two           1       5      9\n",
       "three         2       6     10\n",
       "four          3       7     11"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.stack().unstack(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pivoting “long” to “wide” Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A common way to store multiple time series in databases and CSV is in so-called long or stacked format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  date     item     value\n",
       "0  1959-03-31 00:00:00  realgdp  2710.349\n",
       "1  1959-03-31 00:00:00     infl     0.000\n",
       "2  1959-03-31 00:00:00    unemp     5.800\n",
       "3  1959-06-30 00:00:00  realgdp  2778.801\n",
       "4  1959-06-30 00:00:00     infl     2.340\n",
       "5  1959-06-30 00:00:00    unemp     5.100\n",
       "6  1959-09-30 00:00:00  realgdp  2775.488\n",
       "7  1959-09-30 00:00:00     infl     2.740\n",
       "8  1959-09-30 00:00:00    unemp     5.300\n",
       "9  1959-12-31 00:00:00  realgdp  2785.204"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'date': [\"1959-03-31 00:00:00\", \"1959-03-31 00:00:00\", \"1959-03-31 00:00:00\", \n",
    "                              \"1959-06-30 00:00:00\", \"1959-06-30 00:00:00\", \"1959-06-30 00:00:00\", \n",
    "                              \"1959-09-30 00:00:00\", \"1959-09-30 00:00:00\", \"1959-09-30 00:00:00\", \n",
    "                              \"1959-12-31 00:00:00\"],\n",
    "                    'item': ['realgdp','infl','unemp','realgdp','infl','unemp','realgdp','infl','unemp','realgdp'],\n",
    "                    'value': [2710.349, 0.000, 5.800, 2778.801, 2.340, 5.100, 2775.488, 2.740, 5.300, 2785.204]\n",
    "                    })\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Data is frequently stored this way in relational databases like MySQL as a fixed schema(column names and data types) allows the number of distinct values in the item column to increase or decrease as data is added or deleted in the table.<br>\n",
    "<br>\n",
    "* In the above example date and item would usually be the primary keys (in relational database parlance), offering both relational integrity and easier joins and programmatic queries in many cases.<br>\n",
    "<br>\n",
    "* The downside, of course, is that the data may not be easy to work with in long format; you might prefer to have a DataFrame containing one column per distinct item value indexed by timestamps in the date column. DataFrame’s pivot method performs exactly this transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item                 infl   realgdp  unemp\n",
       "date                                      \n",
       "1959-03-31 00:00:00  0.00  2710.349    5.8\n",
       "1959-06-30 00:00:00  2.34  2778.801    5.1\n",
       "1959-09-30 00:00:00  2.74  2775.488    5.3\n",
       "1959-12-31 00:00:00   NaN  2785.204    NaN"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted = data.pivot('date', 'item', 'value')\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note that pivot is just a shortcut for creating a reshaping with unstack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* So far in this chapter we’ve been concerned with rearranging data.<br>\n",
    "<br>\n",
    "* Filtering, cleaning, and other tranformations are another class of important operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Duplicate rows may be found in a DataFrame for any number of reasons. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "0  one   1\n",
       "1  one   1\n",
       "2  one   2\n",
       "3  two   3\n",
       "4  two   3\n",
       "5  two   4\n",
       "6  two   4"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'k1': ['one'] * 3 + ['two'] * 4,'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The DataFrame method duplicated returns a boolean Series indicating whether each row is a duplicate or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4     True\n",
       "5    False\n",
       "6     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Also, we can use this as a mask for finding duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "1  one   1\n",
       "4  two   3\n",
       "6  two   4"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Other way to get duplicated values is using drop_duplicates which returns a DataFrame where the duplicated array is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "0  one   1\n",
       "2  one   2\n",
       "3  two   3\n",
       "5  two   4"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Also, you can filter duplicates based on a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    k1  k2\n",
       "0  one   1\n",
       "3  two   3"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transforming Data Using a Function or Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For many data sets, you may wish to perform some transformation based on the values in an array, Series, or column in a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider the following hypothetical data collected about customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   birthYear   names\n",
       "0       1984     ali\n",
       "1       1993   Burcu\n",
       "2       1992  Volkan\n",
       "3       1976   Filiz\n",
       "4       1985   Caner\n",
       "5       1978    Emre\n",
       "6       1973   Dilek\n",
       "7       1995   Gamze"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'names': ['ali', 'Burcu', 'Volkan', 'Filiz', 'Caner', \n",
    "                               'Emre', 'Dilek', 'Gamze'],\n",
    "                    'birthYear': [1984, 1993, 1992, 1976, 1985, 1978, 1973, 1995]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* And consider that the following hypothetical data also collected about same customers from different source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ali': 'Istanbul',\n",
       " 'burcu': 'Bursa',\n",
       " 'caner': 'Izmir',\n",
       " 'dilek': 'Antalya',\n",
       " 'emre': 'Izmir',\n",
       " 'filiz': 'Ankara',\n",
       " 'gamze': 'Istanbul',\n",
       " 'volkan': 'Bursa'}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birthPlaces = {'ali': 'Istanbul', 'burcu': 'Bursa', 'volkan': 'Bursa', \n",
    "                  'filiz': 'Ankara', 'caner': 'Izmir', 'emre': 'Izmir',\n",
    "                  'dilek': 'Antalya', 'gamze': 'Istanbul'\n",
    "}\n",
    "birthPlaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose you wanted to add a column to `DataFrame` which indicate the birth place of customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   birthYear   names birthPlace\n",
       "0       1984     ali   Istanbul\n",
       "1       1993   Burcu      Bursa\n",
       "2       1992  Volkan      Bursa\n",
       "3       1976   Filiz     Ankara\n",
       "4       1985   Caner      Izmir\n",
       "5       1978    Emre      Izmir\n",
       "6       1973   Dilek    Antalya\n",
       "7       1995   Gamze   Istanbul"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['birthPlace'] = data['names'].map(str.lower).map(birthPlaces)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can also do same operation with lambda expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Istanbul\n",
       "1       Bursa\n",
       "2       Bursa\n",
       "3      Ankara\n",
       "4       Izmir\n",
       "5       Izmir\n",
       "6     Antalya\n",
       "7    Istanbul\n",
       "Name: names, dtype: object"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['names'].map(lambda x: birthPlaces[x.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Replacing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Filling in missing data with the fillna method can be thought of as a special case of more general value replacement.<br>\n",
    "<br>\n",
    "* While map, as you’ve seen above, can be used to modify a subset of values in an object.<br>\n",
    "<br>\n",
    "* Replace provides a simpler and more flexible way to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1    -999.0\n",
       "2       2.0\n",
       "3    -999.0\n",
       "4   -1000.0\n",
       "5       3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The -999 values might be sentinel values for missing data. \n",
    "* To replace these with NaN values that pandas understands, we can use replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       NaN\n",
       "2       2.0\n",
       "3       NaN\n",
       "4   -1000.0\n",
       "5       3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Notice that replace method return new Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1    -999.0\n",
       "2       2.0\n",
       "3    -999.0\n",
       "4   -1000.0\n",
       "5       3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       NaN\n",
       "2       2.0\n",
       "3       NaN\n",
       "4   -1000.0\n",
       "5       3.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace(-999, np.nan)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Renaming Axis Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Like values in a Series, axis labels can be similarly transformed by a function or mapping of some form to produce new, differently labeled objects.<br>\n",
    "<br>\n",
    "* The axes can also be modified in place without creating a new data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one  two  three  four\n",
       "Istanbul    0    1      2     3\n",
       "Ankara      4    5      6     7\n",
       "Izmir       8    9     10    11"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                 index=['Istanbul', 'Ankara', 'Izmir'],\n",
    "                 columns=['one', 'two', 'three', 'four'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Like a Series, the axis indexes have a map method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ISTANBUL', u'ANKARA', u'IZMIR'], dtype='object')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.map(str.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          one  two  three  four\n",
       "ISTANBUL    0    1      2     3\n",
       "ANKARA      4    5      6     7\n",
       "IZMIR       8    9     10    11"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index = data.index.map(str.upper)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Pandas' `cut` function can be used to group continuous or countable data in to bins.<br>\n",
    "<br>\n",
    "* Discretization is generally a very **bad idea** for statistical analysis, so use this function responsibly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Suppose you have data about a group of people in a study, and you want to group them into discrete age buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. To do so, you have to use cut, a function in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bins = [18, 25, 35, 60,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]\n",
       "Length: 12\n",
       "Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = pd.cut(ages, bins)\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The object pandas returns is a special Categorical object.<br>\n",
    "<br>\n",
    "* You can treat it like an array of strings indicating the bin name; internally it contains a levels array indicating the distinct category names along with a labeling for the ages data in the codes attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]\n",
       "              closed='right',\n",
       "              dtype='interval[int64]')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 25]     5\n",
       "(35, 60]     3\n",
       "(25, 35]     3\n",
       "(60, 100]    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* You can also pass your own bin names by passing a list or array to the labels option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult]\n",
       "Length: 12\n",
       "Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.cut(ages, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A closely related function, qcut, bins the data based on sample quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "quantiles = pd.qcut(data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.666, 3.798]      250\n",
       "(0.039, 0.666]      250\n",
       "(-0.607, 0.039]     250\n",
       "(-3.408, -0.607]    250\n",
       "dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Permutation and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For some data analysis tasks, such as simulation, we need to be able to randomly reorder our data, or draw random values from it. \n",
    "* Calling NumPy's `permutation` function with the length of the sequence you want to permute generates an array with a permuted sequence of integers, which can be used to re-order the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 0, 2, 3])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = np.random.permutation(5)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* That array can then be used in the take function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0   1   2   3\n",
       "0   0   1   2   3\n",
       "1   4   5   6   7\n",
       "2   8   9  10  11\n",
       "3  12  13  14  15\n",
       "4  16  17  18  19"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.arange(5 * 4).reshape(5, 4))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0   1   2   3\n",
       "4  16  17  18  19\n",
       "1   4   5   6   7\n",
       "0   0   1   2   3\n",
       "2   8   9  10  11\n",
       "3  12  13  14  15"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data aggregation and GroupBy operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful features of Pandas is its **GroupBy** functionality. On occasion we may want to perform operations on *groups* of observations within a dataset. For exmaple:<br>\n",
    "* **aggregation**, such as computing the sum of mean of each group, which involves applying a function to each group and returning the aggregated results<br>\n",
    "<br>\n",
    "* **slicing** the DataFrame into groups and then doing something with the resulting slices (*e.g.* plotting)<br>\n",
    "<br>\n",
    "* group-wise **transformation**, such as standardization/normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A common data analysis procedure is the **split-apply-combine** operation, which groups subsets of data together, applies a function to each of the groups, then recombines them into a new data table.<br>\n",
    "<br>\n",
    "* For example, we may want to aggregate our data with with some function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![split-apply-combine](images/groupByVisual.png)\n",
    "<div align=\"right\">*(figure taken from \"Python for Data Analysis\", p.251)*</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*  To get started, here is a very simple small tabular dataset as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      data1     data2 key1 key2\n",
       "0  0.619755 -1.852475    a  one\n",
       "1 -0.990023  0.152956    a  two\n",
       "2  0.527230 -1.175463    b  one\n",
       "3 -1.266556 -0.236212    b  two\n",
       "4  0.277437 -1.481521    a  one"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "    'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "    'data1' : np.random.randn(5), \n",
    "    'data2' : np.random.randn(5)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Suppose you wanted to compute the mean of the data1 column using the groups labels from key1.\n",
    "* There are a number of ways to do this. One is to access data1 and call groupby with the column (a Series) at key1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.SeriesGroupBy object at 0x10b631050>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This grouped variable is now a GroupBy object.\n",
    "* It has not actually computed anything yet except for some intermediate data about the group key df['key1']. \n",
    "* The idea is that this object has all of the information needed to then apply some operation to each of the groups.(lazy evulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For example, to compute group means we can call the GroupBy’s mean method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key1\n",
       "a   -0.030944\n",
       "b   -0.369663\n",
       "Name: data1, dtype: float64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              data1     data2\n",
       "key1 key2                    \n",
       "a    one   0.448596 -1.666998\n",
       "     two  -0.990023  0.152956\n",
       "b    one   0.527230 -1.175463\n",
       "     two  -1.266556 -0.236212"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['key1', 'key2']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The `add_prefix` and `add_suffix` methods can be used to give the columns of the resulting table labels that reflect the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           data1_mean  data2_mean\n",
       "key1 key2                        \n",
       "a    one     0.448596   -1.666998\n",
       "     two    -0.990023    0.152956\n",
       "b    one     0.527230   -1.175463\n",
       "     two    -1.266556   -0.236212"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['key1', 'key2']).mean().add_suffix('_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key1\n",
       "MeanOfa   -0.030944\n",
       "MeanOfb   -0.369663\n",
       "Name: data1, dtype: float64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.mean().add_prefix('MeanOf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The GroupBy object supports iteration, generating a sequence of 2-tuples containing the group name along with the chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "      data1     data2 key1 key2\n",
      "0  0.619755 -1.852475    a  one\n",
      "1 -0.990023  0.152956    a  two\n",
      "4  0.277437 -1.481521    a  one\n",
      "b\n",
      "      data1     data2 key1 key2\n",
      "2  0.527230 -1.175463    b  one\n",
      "3 -1.266556 -0.236212    b  two\n"
     ]
    }
   ],
   "source": [
    "for name, group in df.groupby('key1'): \n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In the case of multiple keys, the first element in the tuple will be a tuple of key values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'one')\n",
      "      data1     data2 key1 key2\n",
      "0  0.619755 -1.852475    a  one\n",
      "4  0.277437 -1.481521    a  one\n",
      "('a', 'two')\n",
      "      data1     data2 key1 key2\n",
      "1 -0.990023  0.152956    a  two\n",
      "('b', 'one')\n",
      "     data1     data2 key1 key2\n",
      "2  0.52723 -1.175463    b  one\n",
      "('b', 'two')\n",
      "      data1     data2 key1 key2\n",
      "3 -1.266556 -0.236212    b  two\n"
     ]
    }
   ],
   "source": [
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print(k1, k2)\n",
    "    print group"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
