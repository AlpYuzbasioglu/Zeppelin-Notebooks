{"paragraphs":[{"text":"%md\n#PySpark Workshop","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>PySpark Workshop</h1>\n"}]},"apps":[],"jobName":"paragraph_1526500159815_1484312616","id":"20180507-083713_1855354539","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:25614"},{"text":"%md\n## Systems check\n\nDo you have a working `spark`?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Systems check</h2>\n<p>Do you have a working <code>spark</code>?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159815_1484312616","id":"20180507-084926_2043145477","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25615"},{"text":"%spark2.pyspark\nspark.version","dateUpdated":"2018-05-16T19:49:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"u'2.2.0.2.6.4.0-91'\n"}]},"apps":[],"jobName":"paragraph_1526500159815_1484312616","id":"20180507-083647_435184395","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25616"},{"text":"%md\nYou can look at these websites:\n\n- [Apache Spark](https://spark.apache.org)\n- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can look at these websites:</p>\n<ul>\n<li><a href=\"https://spark.apache.org\">Apache Spark</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/api/python/\">PySpark Documentation</a></li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1526500159816_1482388872","id":"20180507-132035_832448759","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25617"},{"text":"%md\n## A little about this workshop\n\nWe'll be analysing a Billboard Hot 100 Charts Data.\n\n[Check out the this week chart](https://www.billboard.com/charts/hot-100)\n\n- Each week since 1958, Billboard magazine has released its Hot 100 chart.\n- This list of the 100 most popular songs in the U.S. according to airplay, streams, and physical sales.\n- This dataset contains every weekly list in 1958 to 2016","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>A little about this workshop</h2>\n<p>We'll be analysing a Billboard Hot 100 Charts Data.</p>\n<p><a href=\"https://www.billboard.com/charts/hot-100\">Check out the this week chart</a></p>\n<ul>\n<li>Each week since 1958, Billboard magazine has released its Hot 100 chart.</li>\n<li>This list of the 100 most popular songs in the U.S. according to airplay, streams, and physical sales.</li>\n<li>This dataset contains every weekly list in 1958 to 2016</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1526500159816_1482388872","id":"20180507-084917_1551835436","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25618"},{"text":"%md\nFirst, let's check out the hadoop file system with Zeppelin `sh` interpreter.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>First, let's check out the hadoop file system with Zeppelin <code>sh</code> interpreter.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159816_1482388872","id":"20180507-103837_141121434","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25619"},{"text":"%md\n\nDon't forget to change `userX`","dateUpdated":"2018-05-16T19:49:19+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Don't forget to change <code>userX</code></p>\n"}]},"apps":[],"jobName":"paragraph_1526500159817_1482004123","id":"20180516-090257_1454852207","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25620"},{"text":"%sh\nhadoop fs -ls /user/userX/zpl/","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 2 items\n-rw-r--r--   1 hdfs user4   26658115 2018-05-16 08:45 /user/user4/zpl/billboardHot100.csv\n-rw-r--r--   1 hdfs user4    2379383 2018-05-16 08:47 /user/user4/zpl/trackFeatures.csv\n"}]},"apps":[],"jobName":"paragraph_1526500159817_1482004123","id":"20180507-083701_1371406739","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25621"},{"text":"%md\n### Beginning\n\nLet's load data with spark `read` function, and dive in to it.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Beginning</h3>\n<p>Let's load data with spark <code>read</code> function, and dive in to it.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159817_1482004123","id":"20180507-102246_2088517579","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25622"},{"text":"%spark2.pyspark\n\n# Let's read /user/userX/zpl/billboardHot100.csv with spark csv reader\n# header and inferSchema options will be useful\n\n# billboardDF = ","dateUpdated":"2018-05-16T19:58:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1526500159817_1482004123","id":"20180507-083852_1755309792","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25623"},{"text":"%md\n\nAs you can see, our data in `csv` format, but you can use `read` method with `parquet`, `json`, `orc` and `text`. \n\nAlso we gave extra 2 parameters. Let's check out schema inference works as expected by spark `printSchema` method.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see, our data in <code>csv</code> format, but you can use <code>read</code> method with <code>parquet</code>, <code>json</code>, <code>orc</code> and <code>text</code>.</p>\n<p>Also we gave extra 2 parameters. Let's check out schema inference works as expected by spark <code>printSchema</code> method.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159817_1482004123","id":"20180507-110502_478710225","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25624"},{"text":"%spark2.pyspark\n\n# printSchema()","user":"user0","dateUpdated":"2018-05-16T19:58:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159818_1483158369","id":"20180507-105433_1755798065","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T19:54:03+0000","dateFinished":"2018-05-16T19:54:03+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25625"},{"text":"%md\n\nField names and types looks good except \"**date**\" column but we can change it later. \n\nLet's take a look at rows.\n\nFor displaying data, you can use **spark** `show` method or **ZeppelinContext** show method(`z.show`).","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Field names and types looks good except &ldquo;<strong>date</strong>&rdquo; column but we can change it later.</p>\n<p>Let's take a look at rows.</p>\n<p>For displaying data, you can use <strong>spark</strong> <code>show</code> method or <strong>ZeppelinContext</strong> show method(<code>z.show</code>).</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159818_1483158369","id":"20180507-104821_405010467","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25626"},{"text":"%spark2.pyspark\n\n# show()","user":"user0","dateUpdated":"2018-05-16T19:57:58+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":340,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159818_1483158369","id":"20180508-090638_6233992","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T19:54:52+0000","dateFinished":"2018-05-16T19:54:52+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25627"},{"text":"%spark2.pyspark\n\n# z.show()","user":"user0","dateUpdated":"2018-05-16T19:57:57+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true},"helium":{}},"1":{"graph":{"mode":"table","height":300,"optionOpen":false}}},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159818_1483158369","id":"20180507-102454_1089025036","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T19:55:06+0000","dateFinished":"2018-05-16T19:55:06+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25628"},{"text":"%md\n\nAlso you can use `select` method for projecting some or all fields of a dataframe.\n\nFor giving new names to columns, we will use `alias` function.\n\nAnd we will use `limit` function for limiting result.","user":"user0","dateUpdated":"2018-05-16T19:56:34+0000","config":{"tableHide":false,"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Also you can use <code>select</code> method for projecting some or all fields of a dataframe.</p>\n<p>For giving new names to columns, we will use <code>alias</code> function.</p>\n<p>And we will use <code>limit</code> function for limiting result.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159819_1482773620","id":"20180507-160551_1678388400","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T19:56:18+0000","dateFinished":"2018-05-16T19:56:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25629"},{"text":"%spark2.pyspark\n\n# We can select with .select(df.colName)\n","user":"user0","dateUpdated":"2018-05-16T19:59:17+0000","config":{"editorSetting":{"language":"python"},"colWidth":4,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159819_1482773620","id":"20180508-093317_433627084","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T19:57:32+0000","dateFinished":"2018-05-16T19:57:32+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25630"},{"text":"%spark2.pyspark\n\n# We can also use .select(df[\"colName\"])\n","dateUpdated":"2018-05-16T19:59:23+0000","config":{"colWidth":4,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159819_1482773620","id":"20180508-092743_1079006608","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25631"},{"text":"%spark2.pyspark\n\n# Finally, .select('colName')\n","dateUpdated":"2018-05-16T19:59:56+0000","config":{"colWidth":4,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159820_1480849876","id":"20180516-090639_833978334","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25632"},{"text":"%md\n\nDon't forget that `select` method returns a new dataframe and it's a transformation.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Don't forget that <code>select</code> method returns a new dataframe and it's a transformation.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159820_1480849876","id":"20180508-093921_1040157749","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25633"},{"text":"%md\n\n### Question 1\n\nWhat are the genres in the dataset?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Question 1</h3>\n<p>What are the genres in the dataset?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159820_1480849876","id":"20180508-100415_163434581","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25634"},{"text":"%md\n\nFor answering first question, we can use `distinct` method of spark.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For answering first question, we can use <code>distinct</code> method of spark.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159820_1480849876","id":"20180508-100549_1789325537","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25635"},{"text":"%spark2.pyspark\n\n# Let's select distinct genre from the billboardDF\n","dateUpdated":"2018-05-16T20:00:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159821_1480465127","id":"20180508-100954_1564768568","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25636"},{"text":"%md\n\n### Question 2\n\nHow many different songs does the dataset contains?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Question 2</h3>\n<p>How many different songs does the dataset contains?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159821_1480465127","id":"20180507-131623_1073647555","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25637"},{"text":"%spark2.pyspark\n\n# count() method counts the number of items in a dataframe.\n# Let's see distinct rows of our billboardDF\n","dateUpdated":"2018-05-16T20:01:50+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159821_1480465127","id":"20180507-143205_1072582545","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25638"},{"text":"%md\n\nIs distinct count value the answer we are looking for?\n\nFirst things first, let's drop the duplicate rows.\n\nFor removing duplicates, we can use `dropDuplicates` method.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Is distinct count value the answer we are looking for?</p>\n<p>First things first, let's drop the duplicate rows.</p>\n<p>For removing duplicates, we can use <code>dropDuplicates</code> method.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159821_1480465127","id":"20180507-143648_1426841215","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25639"},{"text":"%spark2.pyspark\n\n# Let's drop duplicate records if exists and count the dataset again\n\n# billboardDF = ","dateUpdated":"2018-05-16T20:03:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159821_1480465127","id":"20180507-143527_1140685276","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25640"},{"text":"%md\n\nWell the dataset does not contains duplicate rows, but probably has duplicate songs.\n\nFor check this let's filter our dataset with \"**Frank Sinatra's My Way**\" song.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Well the dataset does not contains duplicate rows, but probably has duplicate songs.</p>\n<p>For check this let's filter our dataset with &ldquo;<strong>Frank Sinatra's My Way</strong>&rdquo; song.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159822_1481619374","id":"20180507-140435_664460556","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25641"},{"text":"%spark2.pyspark\n\n# We can filter our dataset with .filter(df['colName'] == 'xxx') \n# Let's see the records with the artist is 'Frank Sinatra' and the title is 'My Way' \n\n","dateUpdated":"2018-05-16T20:07:02+0000","config":{"editorSetting":{"language":"python"},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159822_1481619374","id":"20180507-135954_1587650450","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25642"},{"text":"%md\nPlease notice that, `where` is alias for `filter`","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Please notice that, <code>where</code> is alias for <code>filter</code></p>\n"}]},"apps":[],"jobName":"paragraph_1526500159822_1481619374","id":"20180514-121349_1720405946","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25643"},{"text":"%md\n\nAs you can see \"**My Way**\" is in the hot 100 list for 8 different times.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see &ldquo;<strong>My Way</strong>&rdquo; is in the hot 100 list for 8 different times.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159822_1481619374","id":"20180507-152005_1976293907","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25644"},{"text":"%md\n\nWe need a unique column for getting distinct song count.\n\nLet's try out **title** column first.\n\nFor testing, we can filter with \"**My Way**\" song.","user":"user0","dateUpdated":"2018-05-16T20:11:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We need a unique column for getting distinct song count.</p>\n<p>Let's try out <strong>title</strong> column first.</p>\n<p>For testing, we can filter with &ldquo;<strong>My Way</strong>&rdquo; song.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159823_1481234625","id":"20180514-120828_2071839447","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T20:11:43+0000","dateFinished":"2018-05-16T20:11:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25645"},{"text":"%spark2.pyspark\n\n# filter with title == \"My Way\"","dateUpdated":"2018-05-16T20:12:28+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":328,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159823_1481234625","id":"20180514-122510_1261816193","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25646"},{"text":"%md\nAs you noticed, there is several different songs titled as \"**My Way**\"\n\n","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you noticed, there is several different songs titled as &ldquo;<strong>My Way</strong>&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159823_1481234625","id":"20180514-122834_253481047","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25647"},{"text":"%spark2.pyspark\n\n# Let's check how many different artists sang the My Way song\n","dateUpdated":"2018-05-16T20:13:21+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":238,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159823_1481234625","id":"20180507-153547_48128007","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25648"},{"text":"%md\n\nClearly, \"**track_id**\" column is unique for every different song.\n\n\"**track_id**\" is unique \"Spotify ID\" for songs. (We will speak on this\tdetailedly later.)","user":"user0","dateUpdated":"2018-05-16T20:13:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Clearly, &ldquo;<strong>track_id</strong>&rdquo; column is unique for every different song.</p>\n<p>&ldquo;<strong>track_id</strong>&rdquo; is unique &ldquo;Spotify ID&rdquo; for songs. (We will speak on this detailedly later.)</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159824_1491622845","id":"20180514-121806_1750246274","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25649"},{"text":"%md\nBut if you remember, we saw some null values on \"**track_id**\" column when we loaded the dataset.\n\nLet's check out the null values first.\n","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>But if you remember, we saw some null values on &ldquo;<strong>track_id</strong>&rdquo; column when we loaded the dataset.</p>\n<p>Let's check out the null values first.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159824_1491622845","id":"20180514-131812_406581948","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25650"},{"text":"%md\n\nFor checking out the null values, we can use `isNull` and `isNotNull` methods of spark dataframes.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For checking out the null values, we can use <code>isNull</code> and <code>isNotNull</code> methods of spark dataframes.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159824_1491622845","id":"20180514-132200_852486232","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25651"},{"text":"%md\nAlso, we can use `isnull` function from pyspark sql functions.\n","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Also, we can use <code>isnull</code> function from pyspark sql functions.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159824_1491622845","id":"20180514-132331_147977982","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25652"},{"text":"%spark2.pyspark\n\n# We can use df[\"col\"].isNull() and isNotNull() functions as a filter expression\n# Let's print the number of records for each condition\n","dateUpdated":"2018-05-16T20:14:58+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":250,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159825_1491238096","id":"20180514-123709_33922283","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25653"},{"text":"%spark2.pyspark\n\nfrom pyspark.sql.functions import *\n\n# We can also use isnull(\"columnName\") function for counting the number of nulls and not nulls in a dataframe\n# Here we will need to group('') our dataframe on this new column and count() it\n","dateUpdated":"2018-05-16T20:17:14+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":248,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159825_1491238096","id":"20180514-123912_514436083","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25654"},{"text":"%md\n\nAlso, you can use `na.drop` function for omiting rows with null values","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Also, you can use <code>na.drop</code> function for omiting rows with null values</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159825_1491238096","id":"20180515-021902_647031629","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25655"},{"text":"%spark2.pyspark\n\n# na.drop(subset=[\"col1\",\"col2\"]) is useful for dropping rows with null values in specific fields\n# Let's try it with track_id and count the cleansed dataset\n","dateUpdated":"2018-05-16T20:18:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159825_1491238096","id":"20180515-021523_894602772","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25656"},{"text":"%md\n\nGoing back to the question, due to null values, we can not use \"**track_id**\" column for answer we are looking for.\n\nSo, we need to create unique column.\n\nConcatenating **artist name** and **song title** will give us the unique column values.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Going back to the question, due to null values, we can not use &ldquo;<strong>track_id</strong>&rdquo; column for answer we are looking for.</p>\n<p>So, we need to create unique column.</p>\n<p>Concatenating <strong>artist name</strong> and <strong>song title</strong> will give us the unique column values.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159825_1491238096","id":"20180514-132054_1521641759","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25657"},{"text":"%md\nFor adding new column, we can use `withColumn` method.\n\nFor concatenating, we will take advantage of `concat` method.\n\nPlease note that, `concat` accepts only Column types. Therefore we will also use `lit` method for converting string to Column type.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For adding new column, we can use <code>withColumn</code> method.</p>\n<p>For concatenating, we will take advantage of <code>concat</code> method.</p>\n<p>Please note that, <code>concat</code> accepts only Column types. Therefore we will also use <code>lit</code> method for converting string to Column type.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159826_1492392343","id":"20180514-135715_420008738","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25658"},{"text":"%spark2.pyspark\n\n# lets concat artist + \" - \" + title and add it as a new column artist_song\n","dateUpdated":"2018-05-16T20:19:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159826_1492392343","id":"20180516-091057_1455081206","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25659"},{"text":"%md\n\nNow, we can answer the question. \n\nHow many different songs does the dataset contains?\n\nLet's use `distinct` method on our new column and find out the answer.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, we can answer the question.</p>\n<p>How many different songs does the dataset contains?</p>\n<p>Let's use <code>distinct</code> method on our new column and find out the answer.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159826_1492392343","id":"20180514-142044_1154069547","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25660"},{"text":"%spark2.pyspark\n\n# select distinct artists_song and count it to find out\n","dateUpdated":"2018-05-16T20:20:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159826_1492392343","id":"20180508-095738_2105876542","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25661"},{"text":"%md\n\n### Bonus \n\nWhat is the percentage that spotify streamable song contained in the dataset?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Bonus</h3>\n<p>What is the percentage that spotify streamable song contained in the dataset?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159826_1492392343","id":"20180514-142936_1562181097","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25662"},{"text":"%md\n\nWe said that \"**track_id**\" is unique \"Spotify ID\" for songs.\n\nWe used advantages of \"**Spotify API**\" like determining genres. That's why there is a \"**track_id**\" column.\n\nWith \"**track_id**\" value you can find a song on Spotify (https://open.spotify.com/track/track_id)","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We said that &ldquo;<strong>track_id</strong>&rdquo; is unique &ldquo;Spotify ID&rdquo; for songs.</p>\n<p>We used advantages of &ldquo;<strong>Spotify API</strong>&rdquo; like determining genres. That's why there is a &ldquo;<strong>track_id</strong>&rdquo; column.</p>\n<p>With &ldquo;<strong>track_id</strong>&rdquo; value you can find a song on Spotify (https://open.spotify.com/track/track_id)</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159827_1492007594","id":"20180515-022947_771959364","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25663"},{"text":"%md\n\nLet's calculate the ratio of rows with spotify track id","user":"user0","dateUpdated":"2018-05-16T20:22:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's calculate the ratio of rows with spotify track id</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159827_1492007594","id":"20180515-023801_1829091607","dateCreated":"2018-05-16T19:49:19+0000","dateStarted":"2018-05-16T20:22:40+0000","dateFinished":"2018-05-16T20:22:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:25664"},{"text":"%spark2.pyspark\n\n# We can achieve this by taking the allSongsCount and track_id not null songs count to two different variables\n# Then we can calculate the ratio\n","dateUpdated":"2018-05-16T20:24:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159827_1492007594","id":"20180514-133258_937787003","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25665"},{"text":"%md\n\n### Question 3\n\nHow has the genres rise and fall over time?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Question 3</h3>\n<p>How has the genres rise and fall over time?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159828_1490083850","id":"20180515-024847_29800280","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25666"},{"text":"%md\n\nLet's look at how a genre's occurrences and rank on the Hot 100 chart changed over time. \n\nTo do so, Let's create a new column called \"**score**\" which is contains values 1 to 100.","user":"user0","dateUpdated":"2018-05-16T20:26:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's look at how a genre's occurrences and rank on the Hot 100 chart changed over time.</p>\n<p>To do so, Let's create a new column called &ldquo;<strong>score</strong>&rdquo; which is contains values 1 to 100.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159828_1490083850","id":"20180515-025457_290167769","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25667"},{"text":"%spark2.pyspark\n\n# We can create score column with 101 - rank\n# withColumn()\n","dateUpdated":"2018-05-16T20:26:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159828_1490083850","id":"20180515-024328_1325895613","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25668"},{"text":"%md\n\nNow, we can calculate total popularity score of each genre.\n\nLet's group our data by genre column and then aggregate score column. For doing this we will use `groupBy` function of dataframes and `agg` function of Grouped Data type.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, we can calculate total popularity score of each genre.</p>\n<p>Let's group our data by genre column and then aggregate score column. For doing this we will use <code>groupBy</code> function of dataframes and <code>agg</code> function of Grouped Data type.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159828_1490083850","id":"20180515-031023_438623371","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25669"},{"text":"%spark2.pyspark\n\n# .groupBy('colName')\n# .agg(sum('colName'))\n# .alias('aliasName') can be used to rename summation column\n","dateUpdated":"2018-05-16T20:28:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"genre","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"totalScore","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159828_1490083850","id":"20180507-152421_2072126537","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25670"},{"text":"%spark2.pyspark\n\nbillboardDF.groupBy('genre')","dateUpdated":"2018-05-16T19:49:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.group.GroupedData object at 0x1421ad0>\n"}]},"apps":[],"jobName":"paragraph_1526500159829_1489699101","id":"20180516-061941_2060616298","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25671"},{"text":"%md\n\nPlease note that, groupBy function returns a \"**GroupedData Object**\"\n\n\"**GroupedData Object**\" has set of methods for aggregations like `count`, `sum`, `avg`, `max`, `min`.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Please note that, groupBy function returns a &ldquo;<strong>GroupedData Object</strong>&rdquo;</p>\n<p>&ldquo;<strong>GroupedData Object</strong>&rdquo; has set of methods for aggregations like <code>count</code>, <code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159829_1489699101","id":"20180516-062032_874228465","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25672"},{"text":"%md\n\nYou can sort the result by using `sort` function.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can sort the result by using <code>sort</code> function.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159829_1489699101","id":"20180515-031926_1879346994","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25673"},{"text":"%md\n\nNote that, `orderBy` is a alias for sort","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that, <code>orderBy</code> is a alias for sort</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159829_1489699101","id":"20180515-032209_1411291266","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25674"},{"text":"%spark2.pyspark\n\n# Let's see the same aggregation with .sort('colName', ascending=True/False)\n","dateUpdated":"2018-05-16T20:30:33+0000","config":{"editorSetting":{"language":"python"},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"genre","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"totalScore","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159830_1490853347","id":"20180515-032107_729378346","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25675"},{"text":"%spark2.pyspark\n\n# We can also use .sort([\"colName1\", \"colName2\"], ascending=[0,1]) for sorting against multiple columns\n","dateUpdated":"2018-05-16T20:31:55+0000","config":{"editorSetting":{"language":"python"},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"genre","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"totalScore","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159830_1490853347","id":"20180515-032240_1381050517","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25676"},{"text":"%md\n\nYou can visualize the result with \"**Zeppelin**\"","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You can visualize the result with &ldquo;<strong>Zeppelin</strong>&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159830_1490853347","id":"20180515-032534_2111106982","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25677"},{"text":"%spark2.pyspark\n\n# We can create a bar chart for our aggregated data\n","dateUpdated":"2018-05-16T20:32:46+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"genre","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"totalScore","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159830_1490853347","id":"20180515-032502_2002358254","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25678"},{"text":"%md\n\nNow, let's look at the change over the years.\n\nFirst, we need to create \"**year**\" column.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, let's look at the change over the years.</p>\n<p>First, we need to create &ldquo;<strong>year</strong>&rdquo; column.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159830_1490853347","id":"20180515-033423_1053696187","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25679"},{"text":"%spark2.pyspark\n\n# We can calculate the year by formatting our date column for \"yyyy\"\n# to_date casts our date column to date type with format \"dd/MMM/yyyy\"\n","dateUpdated":"2018-05-16T20:34:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159831_1490468598","id":"20180507-134605_1985368613","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25680"},{"text":"%spark2.pyspark\n\n# Now we can check the score changes for each genre in different years\n# A Line Chart could be useful for visualising this data\n","dateUpdated":"2018-05-16T20:36:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"lineChart":{"forceY":false,"lineWithFocus":true}},"commonSetting":{},"keys":[{"name":"year","index":1,"aggr":"sum"}],"groups":[{"name":"genre","index":0,"aggr":"sum"}],"values":[{"name":"score","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159831_1490468598","id":"20180515-034345_750573462","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25681"},{"text":"%md\n\n### Bonus \n\nPretify the result by groupping the years to decades and percantage of scores.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Bonus</h3>\n<p>Pretify the result by groupping the years to decades and percantage of scores.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159831_1490468598","id":"20180515-035941_261560170","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25682"},{"text":"%md\n\nThere is several way for calculating decade that corresponding the year.\n\nI would prefer doing this by using `substr` function to trim and `astype` function to convert data type.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>There is several way for calculating decade that corresponding the year.</p>\n<p>I would prefer doing this by using <code>substr</code> function to trim and <code>astype</code> function to convert data type.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159832_1488544854","id":"20180515-040443_1646300554","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25683"},{"text":"%spark2.pyspark\n\n# substring the year column for the last 2 characters, cast it to int and multiply it by 10\n# add a new column named decade as a result\n","dateUpdated":"2018-05-16T20:40:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159832_1488544854","id":"20180515-040141_109433443","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25684"},{"text":"%md\n\nFor calculating percents of score, we will need total sum of scores.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For calculating percents of score, we will need total sum of scores.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159832_1488544854","id":"20180515-044537_1281129804","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25685"},{"text":"%spark2.pyspark\n\n# Now we can group by decade and find total scores for each decade\n\n# totalScoresDF = \n\n","dateUpdated":"2018-05-16T20:43:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159832_1488544854","id":"20180515-044921_1963192190","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25686"},{"text":"%md\n\nNow, let's assign grouped genre data to another dataframe. ","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, let's assign grouped genre data to another dataframe.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159833_1488160105","id":"20180515-044908_1932321704","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25687"},{"text":"%spark2.pyspark\n\n# Let's group by genre and decade, sum('score') and assign it to another dataframe named genreDF\n\n# genreDF = \n","dateUpdated":"2018-05-16T20:42:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159833_1488160105","id":"20180515-044509_1699822423","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25688"},{"text":"%md\n\nNow, we have 2 different grouped dataframe. For making more readable code we can give aliases to data frames.\n\nLet's join them with `join` method of spark.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now, we have 2 different grouped dataframe. For making more readable code we can give aliases to data frames.</p>\n<p>Let's join them with <code>join</code> method of spark.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159833_1488160105","id":"20180516-051808_844493715","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25689"},{"text":"%spark2.pyspark\n\n# Let's alias our genreDF as t1 and totalScoresDF as t2\n# then left join on decade\n","dateUpdated":"2018-05-16T20:45:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159833_1488160105","id":"20180516-052004_1288873630","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25690"},{"text":"%md\n\nAnd I will use `format_number` for rounding percentages.\n\nFor better looking labels we can make some little changes in \"**decade**\" column.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>And I will use <code>format_number</code> for rounding percentages.</p>\n<p>For better looking labels we can make some little changes in &ldquo;<strong>decade</strong>&rdquo; column.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159834_1489314352","id":"20180515-045303_1697604001","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25691"},{"text":"%spark2.pyspark\n\n# Let's select genre, decade and score/yearTotalScore*100\n# Line chart is a good choise for visualising output\n","dateUpdated":"2018-05-16T20:51:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"lineChart":{}},"commonSetting":{},"keys":[{"name":"decades","index":1,"aggr":"sum"}],"groups":[{"name":"genre","index":0,"aggr":"sum"}],"values":[{"name":"scorePercent","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159834_1489314352","id":"20180515-043911_605485945","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25692"},{"text":"%md\n\n### Question 4\n\nFind the Top 10 artists according to the dataset","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Question 4</h3>\n<p>Find the Top 10 artists according to the dataset</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159834_1489314352","id":"20180515-053150_205624909","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25693"},{"text":"%md \n\nLet's grouped our data by artist and count the scores for finding out the artists that most appearances on the list.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's grouped our data by artist and count the scores for finding out the artists that most appearances on the list.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159834_1489314352","id":"20180507-133615_883896409","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25694"},{"text":"%spark2.pyspark\n# group by artist, find agg(count('score')\n# sort by appearances descending\n# limit the top 10 records\n\n","dateUpdated":"2018-05-16T20:53:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159834_1489314352","id":"20180516-050202_610633900","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25695"},{"text":"%md\n\nWell, if you wonder which artists has more leading in rank 1, we can find out just adding one filter.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Well, if you wonder which artists has more leading in rank 1, we can find out just adding one filter.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159835_1488929603","id":"20180516-083959_695117389","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25696"},{"text":"%spark2.pyspark\n\n# add filter to column rank == 1\n\nz.show(\n    billboardDF\n    .filter(billboardDF['rank'] == 1)\n    .groupBy('artist')\n    .agg(count('title').alias('rank_1_count'))\n    .sort('rank_1_count', ascending=False)\n    .limit(10)\n)","dateUpdated":"2018-05-16T20:54:37+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159835_1488929603","id":"20180516-060425_1239111495","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25697"},{"text":"%md\n\n### Question 5\n\nHow has the song diversity changed over time?","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Question 5</h3>\n<p>How has the song diversity changed over time?</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159835_1488929603","id":"20180516-085055_1352200636","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25698"},{"text":"%md\n\nBefore ending this workshop let's spice it up a little bit and find out the answer with pure `SQL` :)","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Before ending this workshop let's spice it up a little bit and find out the answer with pure <code>SQL</code> :)</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159835_1488929603","id":"20180516-084607_1631025490","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25699"},{"text":"%md\n\nFor running \"**SQL**\", we have to register our data frame as a table with `registerTempTable` function of spark.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For running &ldquo;<strong>SQL</strong>&ldquo;, we have to register our data frame as a table with <code>registerTempTable</code> function of spark.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159836_1487005858","id":"20180516-085314_1517424436","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25700"},{"text":"%spark2.pyspark\n\nbillboardDF.registerTempTable(\"billboard\")","dateUpdated":"2018-05-16T19:49:19+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1526500159836_1487005858","id":"20180516-085650_1989861970","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25701"},{"text":"%md\n\nPlease note that, for running \"**SQL**\" change your Zeppelin interpreter to `%spark2.sql` like in the below.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Please note that, for running &ldquo;<strong>SQL</strong>&rdquo; change your Zeppelin interpreter to <code>%spark2.sql</code> like in the below.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159836_1487005858","id":"20180516-090024_60716832","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25702"},{"text":"%spark2.sql\n\n-- Let's select our new temp table\n","dateUpdated":"2018-05-16T20:56:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159836_1487005858","id":"20180516-085810_1803993547","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25703"},{"text":"%md\nBack to the question","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Back to the question</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159836_1487005858","id":"20180516-091944_1435273983","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25704"},{"text":"%spark2.sql\n\n-- to achieve this we need to find the distinct song counts per year\n\n","dateUpdated":"2018-05-16T20:58:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"lineChart":{}},"commonSetting":{},"keys":[{"name":"year","index":1,"aggr":"sum"}],"groups":[],"values":[{"name":"sum(songCount)","index":0,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159837_1486621109","id":"20180516-090304_1500205026","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25705"},{"text":"%md\n\n### Final","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Final</h3>\n"}]},"apps":[],"jobName":"paragraph_1526500159837_1486621109","id":"20180516-044146_1735316934","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25706"},{"text":"%md\n\nIn this section, we will prepare a dataset for next \"**Machine Learning** Workshop\" ","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In this section, we will prepare a dataset for next &ldquo;<strong>Machine Learning</strong> Workshop&rdquo;</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159837_1486621109","id":"20180516-092142_1025109562","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25707"},{"text":"%spark2.pyspark\n\n# We need to read tracksDF from /user/userX/zpl/trackFeatures.csv file\n\n# tracksDF = ","dateUpdated":"2018-05-16T20:59:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1526500159837_1486621109","id":"20180516-041148_1565478731","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25708"},{"text":"%spark2.pyspark\n\n# Let's see the content","dateUpdated":"2018-05-16T20:59:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159838_1487775356","id":"20180516-041344_1940115615","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25709"},{"text":"%spark2.pyspark\n\n# Now we will enrich our billboardDF with track informations by joining two datasets\n\n# mergedDF = \n","dateUpdated":"2018-05-16T21:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159838_1487775356","id":"20180516-041429_1444090304","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25710"},{"text":"%md\n\nFirst, let's drop some columns that won't be useful anymore.\n\nFor removing columns, we can use `drop` function.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>First, let's drop some columns that won't be useful anymore.</p>\n<p>For removing columns, we can use <code>drop</code> function.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159838_1487775356","id":"20180515-053713_1192436093","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25711"},{"text":"%spark2.pyspark\n\n# We can drop \"rank\",\"year\",\"artist_song\",\"score\", \"id\" \n\n# mergedDF = \n","dateUpdated":"2018-05-16T21:01:32+0000","config":{"colWidth":6,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159838_1487775356","id":"20180507-133133_1580487990","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25712"},{"text":"%md\n\nLet's write our \"**mergedDF**\" data frame to HDFS with `write` method of spark.\n\nAs you remember we can read data in `csv`, `parquet`, `json`, `orc` and `text`. \n\nSame as you can write in these file types.","dateUpdated":"2018-05-16T19:49:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's write our &ldquo;<strong>mergedDF</strong>&rdquo; data frame to HDFS with <code>write</code> method of spark.</p>\n<p>As you remember we can read data in <code>csv</code>, <code>parquet</code>, <code>json</code>, <code>orc</code> and <code>text</code>.</p>\n<p>Same as you can write in these file types.</p>\n"}]},"apps":[],"jobName":"paragraph_1526500159839_1487390607","id":"20180516-052832_1519292833","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25713"},{"text":"%spark2.pyspark\n\n# mergedDF.write.parquet(\"/user/userX/zpl/tracks.parquet\")","dateUpdated":"2018-05-16T21:02:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1526500159839_1487390607","id":"20180516-052812_478018982","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25714"},{"text":"%spark2.pyspark\n","dateUpdated":"2018-05-16T19:49:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1526500159839_1487390607","id":"20180516-092901_1293572275","dateCreated":"2018-05-16T19:49:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:25715"}],"name":"spark_df_workshop","id":"2DD1JYX8Y","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C4U48MY3_spark2:user0:":[],"2CK8A9MEG:shared_process":[],"2CKX8WPU1:user0:":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}